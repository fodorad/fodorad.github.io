In the gaze estimation component of our extended framework, we employed the L2CS-Net method \cite{abdelrahman2022l2cs} to derive accurate gaze information of each participant. 
As a pre-processing step, the face bounding box of the tracked participant from section \ref{subsubsec:participant} and the face landmarks and head pose from section \ref{subsubsec:blink} are used.
The L2CS-Net model predicts the pitch and yaw gaze angles separately, to enhance the precision of individual angle predictions, thereby elevating the overall gaze estimation performance. The effectiveness of the L2CS-Net model  is underscored by its state-of-the-art performance on benchmark datasets, as demonstrated in the original study. 

For estimating the gaze, the video recordings from the Tobii glasses worn by the clinicians are used. This allows for a better view of the participant's face and simplifies the estimation for determining if the participant is looking at the clinician. Given that the clinician is wearing the Tobii glasses (which contains a camera) on his/her face, then, to define if the participant is looking at the clinician, we determine if the participant is looking at the center of the camera. 

In assessing the participant's gaze, L2CS-Net \cite{abdelrahman2022l2cs} default hyper-parameters were employed to derive the gaze vector, characterized by yaw and pitch angles within a spherical coordinate system.  We transformed these gaze angles to pixel space, facilitating the determination of the gaze orientation towards the clinician. Post-transformation, the vector originates from the facial center. The norm of the gaze vector, indicative of the participant's gaze direction, is then analyzed using a threshold of 0.45 to discern whether the participant is directing their attention towards the camera lens. 

We only need to estimate the gaze vector for the participant, as the Tobii glasses already provide gaze information about the clinician. The glasses are equipped with infrared light sources, placed near the eyes, which is reflected off the cornea and other eye components. The eye-tracking cameras on the glasses capture these reflections. As the wearer shifts their eyes and focuses on various objects or points of interest, the eye-tracking system consistently records data, providing real-time information on the person's gaze coordinates for every frame. In this case, to determine if the clinician is looking at the participant, we estimate if the gaze points $(x,y)$ pixel values are within the bounding box of the participant's face.
%(if the lenght of the vector lesser than 0.45 than )
Finally, with this method, if both the clinician and the participant are looking at each other, based on the described methods above, we then count it as an eye contact between the two. 