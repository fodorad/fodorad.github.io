<!DOCTYPE HTML>
<html>
	<head>
		<title>Adam Fodor portfolio</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css">
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
		<link rel="stylesheet" href="../../css/styles.css" />
		<noscript><div class="alert alert-warning" role="alert"><h2>Warning!</h2><p>This website requires JavaScript to function. Please enable JavaScript in your browser settings.</p></div></noscript>
	</head>

	<body class="is-preload">
		<header>
			<div id="menu-header">
				<nav>
					<ul>
						<li><a href="#menu">Menu</a></li>
					</ul>
				</nav>
			</div>

			<nav id="menu">
				<h2>
					Menu
					<span id="themeSwitch" class="theme-icon" aria-label="Toggle Theme"><i class="far fa-moon"></i></span>
				</h2>
				<ul>
					<li><a href="../../index.html">Home</a></li>
					<li><a href="../projects.html">Projects</a></li>
					<li><a href="../blog.html">Blog</a></li>
					<li><a href="../events.html">Events</a></li>
					<li><a href="../teaching.html">Teaching</a></li>
					<li><a href="../gallery.html">Gallery</a></li>
					<li><a href="../about.html">About</a></li>
				</ul>
			</nav>		
		</header>

		<div class="container">
			<div class="paper">
				<h1>Multimodal Sentiment and Personality Perception Under
					Speech: A Comparison of Transformer-based Architectures</h1>
				<p>Ádám Fodor, Rachid R. Saboundji, Julio C. S. Jacques Junior, <br> Sergio Escalera, David Gallardo-Pujol, András Lőrincz</p>
			</div>
			<img class="img-fluid rounded middle" src="../../images/projects/personalitylinmult/cover.png" alt=""/>
			<div class="d-flex mt-4 mb-4">
				<a id="button_contact" target="_blank" href="../../pdf/2021_Fodor_Adam_PMLR_Personality_LinMulT.pdf" class="button mx-3">Paper</a>
				<a id="button_contact" target="_blank" href="https://github.com/fodorad/PersonalityLinMulT" class="button primary mx-3">Code</a>
			</div>

			<div class="row">
				<div class="col"></div>
				<div class="col-6 abstract-container">
					<div class="abstract">
						<h2>Abstract</h2>
						<p>Human-machine, human-robot interaction, and collaboration appear in diverse fields, from
							homecare to Cyber-Physical Systems. Technological development is fast, whereas real-time
							methods for social communication analysis that can measure small changes in sentiment
							and personality states, including visual, acoustic and language modalities are lagging, 
							particularly when the goal is to build robust, appearance invariant, and fair methods. We study
							and compare methods capable of fusing modalities while satisfying real-time and invariant
							appearance conditions. We compare state-of-the-art transformer architectures in sentiment
							estimation and introduce them in the much less explored field of personality perception.
							We show that the architectures perform differently on automatic sentiment and personality perception, 
							suggesting that each task may be better captured/modeled by a particular
							method. Our work calls attention to the attractive properties of the linear versions of
							the transformer architectures. In particular, we show that the best results are achieved
							by fusing the different architectures’ preprocessing methods. However, they pose extreme
							conditions in computation power and energy consumption for real-time computations for
							quadratic transformers due to their memory requirements. In turn, linear transformers pave
							the way for quantifying small changes in sentiment estimation and personality perception
							for real-time social communications for machines and robots.</p>
					</div>
				</div>
				<div class="col"></div>
			</div>

			<h2>Contribution</h2>
				<ul>
					<li>We compare different Transformer-based architectures, with the aim of finding the trade-off between accuracy and efficiency, as well as the best competitive architecture that works well for both sentiment and personality perception;</li>
					<li>We use Action Units as inputs for the visual modality, which are supposed to be invariant to some appearance-based features (e.g., skin-tone or hair-style). Thus, mitigating possible sources of bias toward under-represented groups/categories while promoting cross-dataset/domain/scenario generalization.</li>
					<li>We consider memory and speed parameters as required by real-time processing in human-machine, human-robot interactions, an important goal of future developments.</li>
					<li>The proposed approach was evaluated on different tasks and datasets. The results obtained are similar to those given by state-of-the-art models on the respective tasks and datasets.</li>
				</ul>

			<h2>Proposed method</h2>
				<p>Our method revolves around a cross-modal transformer, with its core being the single-head transformer. 
					This transformer uses an attention model to process input from one modality and transform it into another. 
					We simplify the quadratic transformer to its linear version, as described in related literature. 
					We work with three data modalities: acoustic, visual, and textual components. 
					Each modality undergoes transformation using the other two as sources. 
					The outputs are combined and processed through self-attention to highlight and weigh related elements. 
					This process repeats for each modality before a final prediction is made using a fully-connected layer. 
					Our model builds upon the Multimodal Transformer, which extends the standard Transformer network to accommodate various data types.
					Additionally, we've modified the attention mechanism to a linear complexity alternative.</p>

				<p>Single head of a transformer. Transformers translate one information source
					(e.g., β, here associated with the acoustic modality) to another one (e.g., α, here
					associated with the visual modality). Embeddings indicated by darker striped
					columns can be features derived from the raw data or the outputs of a pretrained deep model. 
					Transformers learn keys (K<sub>β</sub>) and values (V<sub>β</sub>) of modality
					β and the queries (Q<sub>α</sub>) from the α modality. These three quantities denoted by
					lighter striped columns form the core of the attention system of a single head.
					Attention is computed differently in linear and quadratic transformers, sketched
					in the boxes on the right.</p>
				<img class="img-fluid rounded middle" src="../../images/projects/personalitylinmult/head.png" alt=""/>
				
				<p>(a) Standard Multi-head attention unit. (b) Multimodal transformer: source
					modalities i and j are transformed to target modality k. Such two units are
					combined by another transformer network that utilizes self-attention to fuse the
					information pieces to form a branch within the multimodal network before outputting the predicted score(s).</p>
				<img class="img-fluid rounded middle" src="../../images/projects/personalitylinmult/model.png" alt=""/>

			<h2>BibTex</h2>
			<p>If you found our research helpful or influential please consider citing:</p>
			<pre><code class="bibtex">@InProceedings{pmlr-v173-fodor22a,
   title = {Multimodal Sentiment and Personality Perception Under Speech: A Comparison of Transformer-based Architectures},
   author = {Fodor, {\'A}d{\'a}m and Saboundji, Rachid R. and Jacques Junior, Julio C. S. and Escalera, Sergio and Gallardo-Pujol, David and L{\H{o}}rincz, Andr{\'a}s},
   booktitle = {Understanding Social Behavior in Dyadic and Small Group Interactions},
   pages = {218--241},
   year = {2022},
   editor = {Palmero, Cristina and Jacques Junior, Julio C. S. and Clapés, Albert and Guyon, Isabelle and Tu, Wei-Wei and Moeslund, Thomas B. and Escalera, Sergio},
   volume = {173},
   series = {Proceedings of Machine Learning Research},
   month = {16 Oct},
   publisher = {PMLR},
   pdf = {https://proceedings.mlr.press/v173/fodor22a/fodor22a.pdf},
   url = {https://proceedings.mlr.press/v173/fodor22a.html}
}</code></pre>
		</div>

		<div class="d-flex mt-4 mb-4"><a id="button_contact" href="#" class="button mx-3">Details at the Top: Paper</a></div>
		<div id="footer-container"></div>

		<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js"></script>
		<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.7.1/jquery.min.js"></script>	
		<script src="https://kit.fontawesome.com/a8f44bbdfa.js"></script>
		<script src="../../js/menu.js"></script>
		<script src="../../js/pre-load.js"></script>
		<script src="../../js/footer.loader.js"></script>
	</body>
</html>