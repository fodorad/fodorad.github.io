<!DOCTYPE HTML>
<html>
	<head>
		<title>Adam Fodor portfolio</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css">
		<link rel="stylesheet" href="../../css/styles.css" />
		<noscript><div class="alert alert-warning" role="alert"><h2>Warning!</h2><p>This website requires JavaScript to function. Please enable JavaScript in your browser settings.</p></div></noscript>
	</head>

	<body class="is-preload">
		<header>
			<div id="menu-header">
				<nav>
					<ul>
						<li><a href="#menu">Menu</a></li>
					</ul>
				</nav>
			</div>

			<nav id="menu">
				<h2>
					Menu
					<span id="themeSwitch" class="theme-icon" aria-label="Toggle Theme"><i class="far fa-moon"></i></span>
				</h2>
				<ul>
					<li><a href="../../index.html">Home</a></li>
					<li><a href="../projects.html">Projects</a></li>
					<li><a href="../blog.html">Blog</a></li>
					<li><a href="../events.html">Events</a></li>
					<li><a href="../teaching.html">Teaching</a></li>
					<li><a href="../gallery.html">Gallery</a></li>
					<li><a href="../about.html">About</a></li>
				</ul>
			</nav>		
		</header>

		<div class="container">
			<h1>Adaptive, Hybrid Feature Selection (AHFS)</h1>
			<img class="img-fluid rounded" src="../../images/projects/ahfs/paper.png" alt=""/>
			<div class="d-flex mt-4 mb-4">
				<a id="button_contact" href="../../pdf/2021_Fodor_Adam_PR_AHFS.pdf" class="button mx-3">Paper</a>
				<a id="button_contact" href="https://github.com/viharoszsolt/AHFS" class="button primary mx-3">Code</a>
			</div>
			<h1>Abstract</h1>
			<p>This paper deals with the problem of integrating the most suitable feature selection methods for a given
				problem in order to achieve the best feature order. A new, adaptive and hybrid feature selection approach
				is proposed, which combines and utilizes multiple individual methods in order to achieve a more generalized solution.</p>
			<p>Various state-of-the-art feature selection methods are presented in detail with examples
				of their applications and an exhaustive evaluation is conducted to measure and compare the their performance with the proposed approach. 
				Results prove that while the individual feature selection methods
				may perform with high variety on the test cases, the combined algorithm steadily provides noticeably
				better solution.</p>

			<h1>Task and motivation</h1>
				<p>Most real world modeling problems can be formulated as estimation of some numerical value or classifying a given number
					of samples. These problems, more often than not, are very complex and can be defined by tens, hundreds and even thousands
					of variables. The high dimensional data is hard to handle by soft
					computing methods, but in most of the cases many variables are
					highly redundant, noisy and/or irrelevant when solving a specific
					estimation or classification task. The need arises to reduce the dimension of a problem by selecting only the relevant features for
					a given assignment with relatively fast methods compared to the
					highly accurate but costly estimation models.</p>
				<p>Feature Selection (FS)
					methods are doing exactly that. Some of them are able to perform
					calculations faster than others but with the price of losing accuracy, while others work the other way around. Generally, there is
					no universal solution, some methods are more suitable for a given
					assignment than others.</p>

			<h1>Proposed method</h1>
			<div class="row">
				<div class="col d-flex justify-content-center align-items-center">
					<div class="row">
						<p>We propose a new, adaptive and hybrid feature selection approach, which combines and utilizes multiple individual methods
							in order to achieve a more generalized solution. It minimizes the
							shortcomings of each incorporated algorithms by choosing dynamically the most suitable one for a given assignment and dataset.</p>
						<p>The main aim is to ensure with the adaptivity of the
							proposed algorithm at each iteration step of the applied Sequential Forward Search (SFS) iteration. At a certain SFS step a set of already
							selected variables (features) are given and the method evaluates
							each possible extension of this dataset by one additional variable.</p>
						<p>In the state-of-the-art solutions it uses (only) one feature selection measure for selecting an additional variable for the final extension, so, the state-of-the-art algorithms iterate in the space of the
							variables. Adaptivity of the proposed algorithm is realized in such
							a way that at an individual step of the feature selection algorithm it iterates not only in the space of the variables but in
							the space of available features selection techniques, too.</p>
						</div>
					<img class="img-fluid rounded right" src="../../images/projects/ahfs/example.png" alt=""/>
				</div>
			</div>

			<h1>Results</h1>
			<p>There are various applied test cases, the UCI machine learning repository is the probably most frequently applied by the Artificial Intelligence/Machine Learning community. Iris as the
				most frequently used, well-known dataset was selected for having as one of the first classification test assignment. For regression oriented tests another dataset named Housing (also named as
				Boston) was selected as a public benchmark case. In order for having noise free dataset together with also noisy data from the same
				domain Calculated cutting and Measured cutting is applied, respectively. Wind turbine monitoring and Situation detection during special machining are test cases for higher data amount and for high
				data complexity/variety, with various levels of noisy data, incorporating party redundancy, high non-linearity, outliers, non-uniform
				data distribution and many other disturbing, industrial real-life effects.</p>
			<img class="img-fluid rounded middle" src="../../images/projects/ahfs/datasets.png" alt=""/>
			<p>The figure shows the average of the individual feature selection
				performances measured on each assignment. Each line describes
				the performance of a single, individual feature selection method,
				where the x axis shows the number of features used for building
				the model and the y axis shows the normalized model error.
				The overall performance diagram 
				mirrors, that the proposed algorithm significantly outperforms the
				individual methods in general. Furthermore, the biggest difference reveals itself in the case of the first 4 to 25 selected features which
				means that the new method finds the most important features earlier than the other methods</p>
			<div class="row">
				<div class="col d-flex justify-content-center align-items-center">
					<img class="img-fluid rounded left" src="../../images/projects/ahfs/table.png" alt=""/>
					<img class="img-fluid rounded right" src="../../images/projects/ahfs/performance.png" alt=""/>
				</div>
			</div>
			<p>The table on the left mirrors the overall performance (model accuracy) improvement of the proposed algorithm compared to the individual,
				state-of-the-art methods. The rows of the table denote the base of the comparison, in the first
				row (MEAN), the proposed algorithm is compared to the mean performance of the individual methods; in the second row (MIN), it is compared to the minimum of the individual method errors for
				each feature number; and in the third row (MMIFS), it is compared
				to the best individual method, which provided the best model accuracy in general.
				The percentages vary from 139% to 304% with the overall average of 183%, which concludes that the AHFS nearly doubles the accuracy (resulting in around half value for the related modeling
				error) compared to the individual methods, making it a superior
				feature selection algorithm.</p>
				<p>All in all, the proposed AHFS algorithm already proved to be
				superior to other state-of-the-art feature selection methods for
				the reasons that, 1) it is significantly less sensitive to the varying properties of the dataset it is applied to, and 2) it provides
				a significantly better feature order for model building.</p>

			<h1>BibTex</h1>
			<pre><code class="bibtex">@article{VIHAROS2021107932,
   author = {Zsolt János Viharos and Krisztián Balázs Kis and Ádám Fodor and Máté István Büki},
   title = {Adaptive, Hybrid Feature Selection (AHFS)},
   journal = {Pattern Recognition},
   volume = {116},
   pages = {107932},
   year = {2021},
   issn = {0031-3203},
   doi = {https://doi.org/10.1016/j.patcog.2021.107932},
   url = {https://www.sciencedirect.com/science/article/pii/S0031320321001199}
}</code></pre>
		</div>

		<div id="footer-container"></div>

		<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js"></script>
		<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.7.1/jquery.min.js"></script>	
		<script src="https://kit.fontawesome.com/a8f44bbdfa.js"></script>
		<script src="../../js/menu.js"></script>
		<script src="../../js/pre-load.js"></script>
		<script src="../../js/footer.loader.js"></script>
	</body>
</html>