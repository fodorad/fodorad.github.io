<!DOCTYPE HTML>
<html>
	<head>
		<title>Adam Fodor portfolio</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css">
		<link rel="stylesheet" href="../../css/styles.css" />
		<noscript><div class="alert alert-warning" role="alert"><h2>Warning!</h2><p>This website requires JavaScript to function. Please enable JavaScript in your browser settings.</p></div></noscript>
	</head>

	<body class="is-preload">
		<header>
			<div id="menu-header">
				<nav>
					<ul>
						<li><a href="#menu">Menu</a></li>
					</ul>
				</nav>
			</div>

			<nav id="menu">
				<h2>
					Menu
					<span id="themeSwitch" class="theme-icon" aria-label="Toggle Theme"><i class="far fa-moon"></i></span>
				</h2>
				<ul>
					<li><a href="../../index.html">Home</a></li>
					<li><a href="../projects.html">Projects</a></li>
					<li><a href="../blog.html">Blog</a></li>
					<li><a href="../events.html">Events</a></li>
					<li><a href="../teaching.html">Teaching</a></li>
					<li><a href="../gallery.html">Gallery</a></li>
					<li><a href="../about.html">About</a></li>
				</ul>
			</nav>		
		</header>

		<div class="container">
			<div class="paper">
				<h1>Speech de-identification with deep neural networks</h1>
				<p>Ádám Fodor, László Kopácsi, Zoltán Á. Milacski, András Lőrincz</p>
			</div>

			<img class="img-fluid rounded middle fit" src="../../images/projects/deidentification/pipeline.png" alt=""/>
			<div class="d-flex mt-4 mb-4">
				<a id="button_contact" target="_blank" href="../../pdf/2021_Fodor_Adam_CSCS_Deidentification.pdf" class="button mx-3">Paper</a>
				<a id="button_contact" target="_blank" href="../../pdf/2018_Fodor_Adam_Kopacsi_Laszlo_Students_Scientific_Conference_Deidentification.pdf" class="button mx-3">Thesis</a>
				<a id="button_contact" target="_blank" href="https://github.com/lkopi/deidentification" class="button primary mx-3">Code</a>
			</div>
			
			<div class="row">
				<div class="col"></div>
				<div class="col-6 abstract-container">
					<div class="abstract">
						<h2>Abstract</h2>
						<p>Cloud-based speech services are powerful practical tools but the privacy
							of the speakers raises important legal concerns when exposed to the Internet.
							We propose a deep neural network solution that removes personal characteristics 
							from human speech by converting it to the voice of a Text-to-Speech
							(TTS) system before sending the utterance to the cloud. The network learns
							to transcode sequences of vocoder parameters, delta and delta-delta features
							of human speech to those of the TTS engine.
							We evaluated several TTS
							systems, vocoders and audio alignment techniques. We measured the per-
							formance of our method by (i) comparing the result of speech recognition
							on the de-identified utterances with the original texts, (ii) computing the
							Mel-Cepstral Distortion of the aligned TTS and the transcoded sequences,
							and (iii) questioning human participants in A-not-B, 2AFC and 6AFC (Alternative 
							Forced-Choice) tasks. Our approach achieves the level required by
							diverse applications.
						</p>
					</div>
				</div>
				<div class="col"></div>
			</div>

			<h2>Contribution</h2>
				<ul>
					<li>For de-identification, we propose to transform utterances to a generic voice of a Text-to-Speech (TTS) engine, by taking advantage of utterance-text sample pairs.
						We use an end-to-end trainable Deep Neural Network (DNN) to learn the many-to-one VT task. We suggest to learn the mapping at vocoder level.</li>
					<li>We show that the trained network gives rise to tolerable distortions at utterance level by conducting two experiments:
						<ul>
							<li>comparing the outputs of Google’s Automatic Speech Recognition (ASR) system for the original TTS output and the de-identified utterance and measuring the Mel-Cepstral Distortion (MCD).</li>
							<li>To confirm de-identification success, we further performed three kind of perceptual listening studies with human subjects:
								(A-not-B test: distinguishing transformed utterances of different speakers;
								2-Alternative Forced-Choice (2AFC) test: classifying utterances from female/male speakers;
								6-Alternative Forced-Choice (6AFC) test: estimating the number of speakers)
						</ul>
					</li>
					<li>Our proposal is irreversible and it requires only speech-transcript sample pairs for training, which are readily accessible in the literature. We argue that our method performs favorably compared to several baseline methods.</li>
				</ul>

			<h2>Proposed method</h2>
				<p>The proposed method combines preprocessing and deep learning for voice conversion and synthesis. 
					Initially, TTS voices are generated using Festival and aligned with original speech via DTW. Data augmentation, like speed warping, enriches the dataset. 
					Feature transformation employs diverse deep learning architectures such as Dense, ConvNet, and C-BLSTM. 
					The pipeline involves TTS synthesis, alignment, augmentation, and feature transformation, aiming for high-fidelity voice conversion and synthesis.</p>
			
				<div class="row">
					<div class="col d-flex justify-content-center align-items-center">
						<img class="img-fluid rounded left small" src="../../images/projects/deidentification/preprocess.png" alt=""/>
						<img class="img-fluid rounded middle very-small" src="../../images/projects/deidentification/dense.png" alt=""/>
						<img class="img-fluid rounded right small" src="../../images/projects/deidentification/postprocess.png" alt=""/>
					</div>
				</div>

			<h2>Visualization</h2>
			<p>Explore the network's transformations: original inputs on the left, modified results on the right.
				This straightforward presentation unveils the network's refining prowess, offering a practical view of its capabilities.</p>

			<div class="row">
				<div class="col d-flex justify-content-center align-items-center">
					<video controls>
						<source src="../../images/projects/deidentification/0070999.mp4" type="video/mp4">
						Your browser does not support the video tag.
					</video>
					<video controls>
						<source src="../../images/projects/deidentification/0071000.mp4" type="video/mp4">
						Your browser does not support the video tag.
					</video>
				</div>
			</div>
			<div class="row">
				<div class="col d-flex justify-content-center align-items-center">
					<video controls>
						<source src="../../images/projects/deidentification/0071001.mp4" type="video/mp4">
						Your browser does not support the video tag.
					</video>
					<video controls>
						<source src="../../images/projects/deidentification/0071002.mp4" type="video/mp4">
						Your browser does not support the video tag.
					</video>
				</div>
			</div>

			<h2>BibTex</h2>
			<p>If you found our research helpful or influential please consider citing:</p>
			<pre><code class="bibtex">@article{fodor2021deidentification,
   author = {Fodor, Ádám and Kopácsi, László and Milacski, Zoltán Ádám and Lőrincz, András},
   title = {Speech De-identification with Deep Neural Networks},
   journal = {Acta Cybernetica},
   volume = {25},
   number = {2},
   pages = {257-269},
   year = {2021},
   DOI = {10.14232/actacyb.288282},
   url = {https://cyber.bibl.u-szeged.hu/index.php/actcybern/article/view/4178}
}</code></pre>
		</div>

		<div class="d-flex mt-4 mb-4"><a id="button_contact" href="#" class="button mx-3">Details at the Top: Paper</a></div>
		<div id="footer-container"></div>

		<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js"></script>
		<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.7.1/jquery.min.js"></script>	
		<script src="https://kit.fontawesome.com/a8f44bbdfa.js"></script>
		<script src="../../js/menu.js"></script>
		<script src="../../js/pre-load.js"></script>
		<script src="../../js/footer.loader.js"></script>
	</body>
</html>