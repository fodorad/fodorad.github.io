<!DOCTYPE HTML>
<html>
	<head>
		<title>Adam Fodor portfolio</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css">
		<link rel="stylesheet" href="../../css/styles.css" />
		<noscript><div class="alert alert-warning" role="alert"><h2>Warning!</h2><p>This website requires JavaScript to function. Please enable JavaScript in your browser settings.</p></div></noscript>
	</head>

	<body class="is-preload">
		<header>
			<div id="menu-header">
				<nav>
					<ul>
						<li><a href="#menu">Menu</a></li>
					</ul>
				</nav>
			</div>

			<nav id="menu">
				<h2>
					Menu
					<span id="themeSwitch" class="theme-icon" aria-label="Toggle Theme"><i class="far fa-moon"></i></span>
				</h2>
				<ul>
					<li><a href="../../index.html">Home</a></li>
					<li><a href="../projects.html">Projects</a></li>
					<li><a href="../blog.html">Blog</a></li>
					<li><a href="../events.html">Events</a></li>
					<li><a href="../teaching.html">Teaching</a></li>
					<li><a href="../gallery.html">Gallery</a></li>
					<li><a href="../about.html">About</a></li>
				</ul>
			</nav>		
		</header>

		<div class="container">
			<h1>LinMulT: General-Purpose Multimodal Transformer with Linear Complexity Attention Mechanism</h1>
			<img class="img-fluid rounded middle" src="../../images/projects/linmult/LinMulT.jpg" alt=""/>
			<div class="d-flex mt-4 mb-4">
				<a id="button_contact" href="personalitylinmult" class="button mx-3">Project 1: PersonalityLinMulT</a>
				<a id="button_contact" href="blinklinmult" class="button mx-3">Project 2: BlinkLinMulT</a>
				<a id="button_contact" href="https://github.com/fodorad/LinMulT" class="button primary mx-3">Code</a>
			</div>

			<h1>Motivation</h1>
				<p>This repository presents LinMulT, a general-purpose Multimodal Transformer with Linear complexity attention mechanism, offering a versatile solution for various multimodal learning tasks.  
				   It seamlessly handles multiple modalities, be it vectors or sequences, and is trainable for both classification tasks and sequential predictions.</p>

			<h1>Setup</h1>
			<h2>Install package from PyPI</h2>
				<pre><code>pip install linmult</code></pre>

			<h2>Install package for development</h2>
				<pre><code>git clone https://github.com/fodorad/LinMulT
cd LinMulT
pip install -e .
pip install -U -r requirements.txt
python -m unittest</code></pre>

			<h1>Quick start</h1>
			<h2>Example 1:</h2>		
				<p>Simple transformer encoder with linear attention. The forward pass is performed using an input sequence.</p>
				<pre><code>import torch
from linmult import LinT

# input shape: (batch_size, time_dimension, feature_dimension)
x = torch.rand((32, 15, 1024), device='cuda')
model = LinT(input_modality_channels=1024, output_dim=5).cuda()
y_pred_seq = model(x)

# output shape: (batch_size, time_dimension, output_dimension)
assert y_pred_seq.size() == torch.Size([32, 15, 5])
</code></pre>

			<h2>Example 2:</h2>
				<p>Multimodal Transformer with Linear Attention. The forward pass is performed using 2 input sequences. Both input sequences have the same time dimension.</p>
				<pre><code>import torch
from linmult import LinMulT

# input shape: (batch_size, time_dimension, feature_dimension)
x_1 = torch.rand((32, 15, 1024), device='cuda')
x_2 = torch.rand((32, 15, 160), device='cuda')
model = LinMulT(input_modality_channels=[1024, 160], output_dim=5).cuda()
y_pred_cls, y_pred_seq = model([x_1, x_2])

# 1. output shape: (batch_size, output_dimension)
assert y_pred_cls.size() == torch.Size([32, 5])

# 2. output shape: (batch_size, time_dimension, output_dimension)
assert y_pred_seq.size() == torch.Size([32, 15, 5])
</code></pre>

			<h2>Example 3:</h2>
				<p>Multimodal Transformer with Linear Attention. The forward pass is performed using 3 input sequences with different time dimensions.</p>
				<pre><code>import torch
from linmult import LinMulT

# input shape: (batch_size, time_dimension, feature_dimension)
x_1 = torch.rand((16, 1500, 25), device='cuda')
x_2 = torch.rand((16, 450, 35), device='cuda')
x_3 = torch.rand((16, 120, 768), device='cuda')
model = LinMulT(input_modality_channels=[25, 35, 768],
                output_dim=5,
                add_time_collapse=True,
                add_self_attention_fusion=False).cuda()
y_pred_cls = model([x_1, x_2, x_3])

# output shape: (batch_size, output_dimension)
assert y_pred_cls.size() == torch.Size([16, 5])
</code></pre>
				
			<h1>Similar projects using LinMulT</h1>
				<h2>(2023) BlinkLinMulT</h2>
					<p>LinMulT is trained for blink presence detection and eye state recognition tasks. Our results demonstrate comparable or superior performance compared to state-of-the-art models on 2 tasks, using 7 public benchmark databases.</p>
					<ul>
						<li><strong>paper</strong>: BlinkLinMulT: Transformer-based Eye Blink Detection</li>
						<li><strong>code</strong>: https://github.com/fodorad/BlinkLinMulT</li>
					</ul>

				<h2>(2022) PersonalityLinMulT</h2>
					<p>LinMulT is trained for Big Five personality trait estimation using the First Impressions V2 dataset and sentiment estimation using the MOSI and MOSEI datasets.</p>
					<ul>
						<li><strong>paper</strong>: Multimodal Sentiment and Personality Perception Under Speech: A Comparison of Transformer-based Architectures</li>
						<li><strong>code</strong>: https://github.com/fodorad/PersonalityLinMulT</li>
					</ul>


			<h1>BibTex</h1>
				<p>If you found our research helpful or influential please consider citing:</p>
					
				<h3>(2023) LinMulT for blink presence detection and eye state recognition</h3>
				<pre><code class="bibtex">@article{blinklinmult-fodor23,
   title = {BlinkLinMulT: Transformer-based Eye Blink Detection},
   author = {Fodor, {\'A}d{\'a}m and Fenech, Kristian and L{\H{o}}rincz, Andr{\'a}s},
   journal = {...}
   pages = {1--19},
   year = {2023}
}</code></pre>
				<h3>(2022) LinMulT for personality trait and sentiment estimation</h3>
				<pre><code class="bibtex">@InProceedings{pmlr-v173-fodor22a,
   title = {Multimodal Sentiment and Personality Perception Under Speech: A Comparison of Transformer-based Architectures},
   author = {Fodor, {\'A}d{\'a}m and Saboundji, Rachid R. and Jacques Junior, Julio C. S. and Escalera, Sergio and Gallardo-Pujol, David and L{\H{o}}rincz, Andr{\'a}s},
   booktitle = {Understanding Social Behavior in Dyadic and Small Group Interactions},
   pages = {218--241},
   year = {2022},
   editor = {Palmero, Cristina and Jacques Junior, Julio C. S. and Clap√©s, Albert and Guyon, Isabelle and Tu, Wei-Wei and Moeslund, Thomas B. and Escalera, Sergio},
   volume = {173},
   series = {Proceedings of Machine Learning Research},
   month = {16 Oct},
   publisher = {PMLR},
   pdf = {https://proceedings.mlr.press/v173/fodor22a/fodor22a.pdf},
   url = {https://proceedings.mlr.press/v173/fodor22a.html}
}</code></pre>

			<h1>Acknowledgement</h1>
			<p>The code is inspired by the following two materials:</p>
			<h2>Multimodal Transformer</h2>
			<ul>
				<li><strong>paper</strong>: Multimodal Transformer for Unaligned Multimodal Language Sequences (1906.00295)</li>
				<li><strong>code</strong>: https://github.com/yaohungt/Multimodal-Transformer</li>
			</ul>	
			<h2>Linear Attention</h2>
			<ul>
				<li><strong>paper</strong>: Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention (2006.16236)</li>
				<li><strong>code</strong>: https://github.com/idiap/fast-transformers</li>
			</ul>
		</div>

		<div id="footer-container"></div>

		<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js"></script>
		<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.7.1/jquery.min.js"></script>	
		<script src="https://kit.fontawesome.com/a8f44bbdfa.js"></script>
		<script src="../../js/menu.js"></script>
		<script src="../../js/pre-load.js"></script>
		<script src="../../js/footer.loader.js"></script>
	</body>
</html>